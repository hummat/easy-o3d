{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963d9db9",
   "metadata": {},
   "source": [
    "# Point cloud registration with Easy Open3D\n",
    "In this tutorial we will use the **Easy Open3D** package to register Suzanne, the monkey head mascot from [Blender](https://www.blender.org), to (1) a complete scene point cloud where she is sitting on a chair and (2) a partial point cloud of the same scene in form of an RGB-D image. The result will be her pose, i.e. her rotation and translation in world coordinates in this scene.\n",
    "![](./tests/test_data/test_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package functionality\n",
    "from easy_o3d import utils\n",
    "from easy_o3d.registration import IterativeClosestPoint, FastGlobalRegistration, RANSAC, ICPTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa69908",
   "metadata": {},
   "source": [
    "## 1. Registration on complete data\n",
    "### 1.1 Load and visualize point clouds\n",
    "Let's start by loading the `source` (Suzanne) and `target` (the scene) point clouds from file. Both are stored in the PLY file format as meshes, so we need to sample points from the mesh surfaces to obtain point clouds. We will also load the known ground truth pose in world coordinates to evaluate the fit of the registration results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df81eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth pose. It is stored in a JSON file which must contain keys\n",
    "# containing \"rot\" and \"trans\" substrings.\n",
    "gt_path = \"tests/test_data/ground_truth_pose.json\"\n",
    "gt_pose = utils.eval_transformation_data(gt_path)\n",
    "\n",
    "# Load source and target meshes from PLY files and sample points from their surface.\n",
    "source_path = \"tests/test_data/suzanne.ply\"\n",
    "source = utils.eval_data(data=source_path, number_of_points=10000)\n",
    "\n",
    "target_path = \"tests/test_data/suzanne_on_chair.ply\"\n",
    "target = utils.eval_data(data=target_path, number_of_points=100000)\n",
    "\n",
    "# Visualize data\n",
    "utils.draw_geometries(geometries=[source, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f665f2",
   "metadata": {},
   "source": [
    "### 1.2 Process point clouds\n",
    "Before we run the registration algorithms, it is almost always a good idea to pre-process the data. Through voxel downsampling, we can reduce the computation time and facilitate the finding of correspondences between source and target. We also compute FPFH feature which are needed for the global registration (initialization) phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e951bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_down = utils.process_point_cloud(point_cloud=source,\n",
    "                                        downsample=utils.DownsampleTypes.VOXEL,\n",
    "                                        downsample_factor=0.01)\n",
    "\n",
    "_, source_feature = utils.process_point_cloud(point_cloud=source_down,\n",
    "                                              compute_feature=True,\n",
    "                                              search_param_knn=100,\n",
    "                                              search_param_radius=0.05)\n",
    "\n",
    "target_down = utils.process_point_cloud(point_cloud=target,\n",
    "                                        downsample=utils.DownsampleTypes.VOXEL,\n",
    "                                        downsample_factor=0.01)\n",
    "\n",
    "_, target_feature = utils.process_point_cloud(point_cloud=target_down,\n",
    "                                              compute_feature=True,\n",
    "                                              search_param_knn=100,\n",
    "                                              search_param_radius=0.05)\n",
    "\n",
    "# Visualize data\n",
    "utils.draw_geometries(geometries=[source_down, target_down])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d06d469",
   "metadata": {},
   "source": [
    "### 1.3 Initialization: RANSAC\n",
    "Registration algorithms are commonly categorized into _global_ and _local_, because finding an initial match and an accurate one comes with a different set of challenges. Here we will use the **RANSAC** global registration algorithm to find an initial pose, which we will subsequently refine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd600bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ransac = RANSAC()\n",
    "ransac_result = ransac.run(source=source_down,\n",
    "                           target=target_down,\n",
    "                           source_feature=source_feature,\n",
    "                           target_feature=target_feature,\n",
    "                           draw=True,\n",
    "                           overwrite_colors=True)\n",
    "\n",
    "error_rot, error_trans = utils.get_transformation_error(ransac_result.transformation, gt_pose)\n",
    "print(\"Error rotation [deg]:\", error_rot, \"Error translation [m]:\", error_trans)\n",
    "print(\"Rotation estimate:\")\n",
    "print(ransac_result.transformation[:3, :3])\n",
    "print(\"Translation estimate:\", ransac_result.transformation[:3, 3])\n",
    "print(\"Runtime [s]:\", ransac_result.runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563912a",
   "metadata": {},
   "source": [
    "### 1.4 Initialization: Fast Global Registration\n",
    "The second supported initializer is **Fast Global Registration** which was developed more recently and promises to be faster than RANSAC or more precise, if given the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgr = FastGlobalRegistration()\n",
    "fgr_result = fgr.run(source=source_down,\n",
    "                     target=target_down,\n",
    "                     source_feature=source_feature,\n",
    "                     target_feature=target_feature,\n",
    "                     draw=True,\n",
    "                     overwrite_colors=True)\n",
    "\n",
    "error_rot, error_trans = utils.get_transformation_error(fgr_result.transformation, gt_pose)\n",
    "print(\"Error rotation [deg]:\", error_rot, \"Error translation [m]:\", error_trans)\n",
    "print(\"Rotation estimate:\")\n",
    "print(fgr_result.transformation[:3, :3])\n",
    "print(\"Translation estimate:\", fgr_result.transformation[:3, 3])\n",
    "print(\"Runtime [s]:\", fgr_result.runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0aef48",
   "metadata": {},
   "source": [
    "Because initialization can be sensitive to external factors, we can simply run it multiple times and pick the best result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cad348",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgr = FastGlobalRegistration()\n",
    "fgr_result = fgr.run_n_times(source=source_down,\n",
    "                             target=target_down,\n",
    "                             source_feature=source_feature,\n",
    "                             target_feature=target_feature,\n",
    "                             n_times=3)\n",
    "\n",
    "error_rot, error_trans = utils.get_transformation_error(fgr_result.transformation, gt_pose)\n",
    "print(\"Error rotation [deg]:\", error_rot, \"Error translation [m]:\", error_trans)\n",
    "print(\"Rotation estimate:\")\n",
    "print(fgr_result.transformation[:3, :3])\n",
    "print(\"Translation estimate:\", fgr_result.transformation[:3, 3])\n",
    "print(\"Runtime [s]:\", fgr_result.runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71dfa7c",
   "metadata": {},
   "source": [
    "### 1.5 Refinement: Iterative Closest Point\n",
    "Now that we have an initial guess of the pose of Suzanne (which is already very good for this toy example), we can refine it using the **Iterative Closest Point** algorithm. There are multiple variants and we will make use of the _point-to-plane_ formulation because it tends to be more accurate than _point-to-point_ (albeit sometimes less robust) and we already have computed the required point normals as prerequesits of the FPFH features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392dbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "icp = IterativeClosestPoint(estimation_method=ICPTypes.PLANE)\n",
    "result_icp = icp.run(source=source_down,\n",
    "                     target=target_down,\n",
    "                     init=ransac_result.transformation,  # Plug in our initial estimate\n",
    "                     draw=True,\n",
    "                     overwrite_colors=True)\n",
    "\n",
    "error_rot, error_trans = utils.get_transformation_error(result_icp.transformation, gt_pose)\n",
    "print(\"Error rotation [deg]:\", error_rot, \"Error translation [m]:\", error_trans)\n",
    "print(\"Rotation estimate:\")\n",
    "print(result_icp.transformation[:3, :3])\n",
    "print(\"Translation estimate:\", result_icp.transformation[:3, 3])\n",
    "print(\"Runtime [s]:\", result_icp.runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd44fd0",
   "metadata": {},
   "source": [
    "The quality of the registration result depends on a number of parameters, most notably the _maximum correspondence distance_, determining which distance a point pair in the source and target point cloud is allowed to have at most to be concidered as correspondence. We can alleviate this hyperparameter problem somewhat by running the algorithm with multiple correspondence distances in sequence from large to small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9166b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the original source and target point clouds as they will be downsampled\n",
    "# during multi-scale registration.\n",
    "icp = IterativeClosestPoint(estimation_method=ICPTypes.PLANE)\n",
    "result_icp = icp.run_multi_scale(source=source,  \n",
    "                                 target=target,\n",
    "                                 init=ransac_result.transformation,\n",
    "                                 draw=True,\n",
    "                                 overwrite_colors=True)\n",
    "\n",
    "error_rot, error_trans = utils.get_transformation_error(result_icp.transformation, gt_pose)\n",
    "print(\"Error rotation [deg]:\", error_rot, \"Error translation [m]:\", error_trans)\n",
    "print(\"Rotation estimate:\")\n",
    "print(result_icp.transformation[:3, :3])\n",
    "print(\"Translation estimate:\", result_icp.transformation[:3, 3])\n",
    "print(\"Runtime [s]:\", result_icp.runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f2ef5",
   "metadata": {},
   "source": [
    "## 2. Changing the target: RGB-D images\n",
    "Let's now move to the more relatistic (and challenging) scenario of partial target point clouds in the form of RGB-D images. RGB-D images can be mapped from 2D to 3D space using the intrinsic camera parameters and transformed into the world coordinate frame using the extrinsic camera parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f4377",
   "metadata": {},
   "source": [
    "### 2.1 Load camera parameters from BlenderProc BopWriter\n",
    "[BlenderProc](https://github.com/DLR-RM/BlenderProc) is a an awesome open source synthetic training data generator based on Blender targeted at machine learning applications. It was used to generate the synthetic RGB-D test data for the Easy Open3D package. It can store the generated data in the [Bop Challenge](https://bop.felk.cvut.cz/home) data format, for which various readers are implemented in Easy Open3D for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b312fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_id = 20\n",
    "path_to_scene_camera_json = \"tests/test_data/bop_data/obj_of_interest/train_pbr/000000/scene_camera.json\"\n",
    "path_to_camera_json = \"tests/test_data/bop_data/obj_of_interest/camera.json\"\n",
    "output_path = \"tests/test_data\"\n",
    "\n",
    "camera_parameters = utils.get_camera_parameters_from_blenderproc_bopwriter(path_to_scene_camera_json,\n",
    "                                                                           path_to_camera_json,\n",
    "                                                                           scene_id)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f8f71",
   "metadata": {},
   "source": [
    "### 2.2 Load, process and visualize the RGB-D image\n",
    "Having obtained the intrinsic and extrinsic camera parameters of the scene, we can now convert the RGB-D image to a colored point cloud. The partial scene point cloud is much more challenging to solve so we can make use of the [`hyperopt`](https://github.com/hummat/easy-o3d/blob/master/scripts/hyperopt.py) script to find good set of hyperparameters for processing and registration. Due to the large number of hyperparameters and their interaction, this is quite challenging and tedious to do by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to color and depth image\n",
    "color = f\"tests/test_data/bop_data/obj_of_interest/train_pbr/000000/rgb/{str(scene_id).zfill(6)}.png\"\n",
    "depth = f\"tests/test_data/bop_data/obj_of_interest/train_pbr/000000/depth/{str(scene_id).zfill(6)}.png\"\n",
    "\n",
    "# Load the RGB-D image and convert it to a point cloud\n",
    "target_rgbd = utils.eval_data(data=[color, depth],\n",
    "                              camera_intrinsic=camera_parameters.intrinsic,\n",
    "                              camera_extrinsic=camera_parameters.extrinsic,\n",
    "                              depth_trunc=3.0)  # Truncate depth at 3m\n",
    "\n",
    "# Downsample, remove outlier, estimate normals\n",
    "target_rgbd = utils.process_point_cloud(point_cloud=target_rgbd,\n",
    "                                        downsample=utils.DownsampleTypes.VOXEL,\n",
    "                                        downsample_factor=0.007579519638230087,\n",
    "                                        remove_outlier=utils.OutlierTypes.STATISTICAL,\n",
    "                                        outlier_std_ratio=3.0,\n",
    "                                        estimate_normals=True,\n",
    "                                        search_param=utils.SearchParamTypes.RADIUS,\n",
    "                                        search_param_radius=0.012467595584023949)\n",
    "\n",
    "_, target_rgbd_feature = utils.process_point_cloud(point_cloud=target_rgbd,\n",
    "                                                   compute_feature=True,\n",
    "                                                   search_param=utils.SearchParamTypes.RADIUS,\n",
    "                                                   search_param_radius=0.06968346743904905)\n",
    "\n",
    "# Process source again to correspond to target voxel size\n",
    "source_path = \"tests/test_data/suzanne.ply\"\n",
    "source = utils.eval_data(data=source_path, number_of_points=8586)\n",
    "\n",
    "source_down = utils.process_point_cloud(point_cloud=source,\n",
    "                                        downsample=utils.DownsampleTypes.VOXEL,\n",
    "                                        downsample_factor=0.008829352302399583,\n",
    "                                        estimate_normals=True,\n",
    "                                        recalculate_normals=True,\n",
    "                                        search_param=utils.SearchParamTypes.KNN,\n",
    "                                        search_param_knn=47)\n",
    "\n",
    "_, source_feature = utils.process_point_cloud(point_cloud=source_down,\n",
    "                                              compute_feature=True,\n",
    "                                              search_param=utils.SearchParamTypes.RADIUS,\n",
    "                                              search_param_radius=0.06968346743904905)\n",
    "\n",
    "utils.draw_geometries(geometries=[source_down, target_rgbd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5c9c8",
   "metadata": {},
   "source": [
    "### 2.3 Initialization: RANSAC\n",
    "We use the RANSAC algorithm to find an initial pose estimate using the best hyperparameters found during optimization. Still, this initialization can somtetimes fail. In this case, either use the `run_n_times` functionality or simply rerun this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f38389",
   "metadata": {},
   "outputs": [],
   "source": [
    "ransac = RANSAC(max_iteration = 107323,\n",
    "                max_correspondence_distance=0.029175728431991857,\n",
    "                confidence = 0.9,\n",
    "                similarity_threshold = 0.7)\n",
    "ransac_result = ransac.run(source=source_down,\n",
    "                           target=target_rgbd,\n",
    "                           source_feature=source_feature,\n",
    "                           target_feature=target_rgbd_feature,\n",
    "                           draw=True,\n",
    "                           overwrite_colors=True)\n",
    "\n",
    "error_rot, error_trans = utils.get_transformation_error(ransac_result.transformation, gt_pose)\n",
    "print(\"Error rotation [deg]:\", error_rot, \"Error translation [m]:\", error_trans)\n",
    "print(\"Rotation estimate:\")\n",
    "print(ransac_result.transformation[:3, :3])\n",
    "print(\"Translation estimate:\", ransac_result.transformation[:3, 3])\n",
    "print(\"Runtime [s]:\", ransac_result.runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17357ad6",
   "metadata": {},
   "source": [
    "### 2.4 Refinement: Iterative Closest Point\n",
    "To make sure the ICP algorithm doesn't get confused we can crop the target around the initial source position. This will fail if the initial position is extremely inaccurate, though ICP will fail in such a case anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310568e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "icp = IterativeClosestPoint(max_iteration=100,\n",
    "                            max_correspondence_distance=0.01)\n",
    "result_icp = icp.run(source=source_down,\n",
    "                     target=target_rgbd,\n",
    "                     init=ransac_result.transformation,\n",
    "                     crop_target_around_source=True,\n",
    "                     crop_scale=1.5,\n",
    "                     draw=True,\n",
    "                     overwrite_colors=True)\n",
    "\n",
    "error_rot, error_trans = utils.get_transformation_error(result_icp.transformation, gt_pose)\n",
    "print(\"Error rotation [deg]:\", error_rot, \"Error translation [m]:\", error_trans)\n",
    "print(\"Rotation estimate:\")\n",
    "print(result_icp.transformation[:3, :3])\n",
    "print(\"Translation estimate:\", result_icp.transformation[:3, 3])\n",
    "print(\"Runtime [s]:\", result_icp.runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the final pose again using the complete and colore scene\n",
    "icp.draw_registration_result(source=source, target=target_rgbd, pose=result_icp.transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f35d5",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "This concludes our little tutorial. There is still a lot of functionality that didn't make it in so feel free to have a look around."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
